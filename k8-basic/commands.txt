kubectl run redis --image=redis --namespace=finance --dry-run=client -o yaml > pod-finanace.yaml
kubectl get pods --all-namespaces

While you would be working mostly the declarative way - using definition files, imperative commands can help in getting one-time tasks done quickly, as well as generate a definition template easily. This would help save a considerable amount of time during your exams.

Before we begin, familiarize yourself with the two options that can come in handy while working with the below commands:

--dry-run: By default, as soon as the command is run, the resource will be created. If you simply want to test your command, use the --dry-run=client option. This will not create the resource. Instead, tell you whether the resource can be created and if your command is right.

-o yaml: This will output the resource definition in YAML format on the screen.



Use the above two in combination along with Linux output redirection to generate a resource definition file quickly, that you can then modify and create resources as required, instead of creating the files from scratch.



kubectl run nginx --image=nginx --dry-run=client -o yaml > nginx-pod.yaml



POD
Create an NGINX Pod

kubectl run nginx --image=nginx



Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)

kubectl run nginx --image=nginx --dry-run=client -o yaml



Deployment
Create a deployment

kubectl create deployment --image=nginx nginx



Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)

kubectl create deployment --image=nginx nginx --dry-run -o yaml



Generate Deployment with 4 Replicas

kubectl create deployment nginx --image=nginx --replicas=4



You can also scale deployment using the kubectl scale command.

kubectl scale deployment nginx --replicas=4



Another way to do this is to save the YAML definition to a file and modify

kubectl create deployment nginx --image=nginx--dry-run=client -o yaml > nginx-deployment.yaml



You can then update the YAML file with the replicas or any other field before creating the deployment.



Service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors)

Or

kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml (This will not use the pods' labels as selectors; instead it will assume selectors as app=redis. You cannot pass in selectors as an option. 
So it does not work well if your pod has a different label set. So generate the file and modify the selectors before creating the service)



Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes:

kubectl expose pod nginx --port=80 --name nginx-service --type=NodePort --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors, but you cannot specify the node port. You have to generate a definition file and then add the node port in manually before creating the service with the pod.)

Or

kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml

(This will not use the pods' labels as selectors)

Both the above commands have their own challenges. While one of it cannot accept a selector the other cannot accept a node port. I would recommend going with the `kubectl expose` command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service.



kubectl run nginx-pod --image=nginx:alpine


kubectl run redis --image=redis:alpine --dry-run=client -o yaml > pod.yaml

controlplane ~ ➜  vi pod.yaml 

controlplane ~ ➜  kubectl create -f pod.yaml 

Cluster IP 
kubectl expose pod redis --port=6379 --name redis-service

Create depolyment in command
kubectl create deployment --image=kodekloud/webapp-color webapp --replicas=3

create pod and expose port in 8080
kubectl run custom-nginx --image=nginx --port=8080

create namespace
kubectl create namespace dev-ns

create new deployment in dev-ns namespace
kubectl create deployment redis-deploy --namespace dev-ns --image=redis --replicas=2
deployment.apps/redis-deploy created


Create a pod called httpd using the image httpd:alpine in the default namespace. 
Next, create a service of type ClusterIP by the same name (httpd). The target port for the service should be 80
controlplane ~ ➜  kubectl run httpd --image=httpd:alpine --port=80 --expose
service/httpd created
pod/httpd created


A copy of the file with your changes is saved in a temporary location as shown above.

You can then delete the existing pod by running the command:

kubectl delete pod webapp



Then create a new pod with your changes using the temporary file

kubectl create -f /tmp/kubectl-edit-ccvrq.yaml



2. The second option is to extract the pod definition in YAML format to a file using the command

kubectl get pod webapp -o yaml > my-new-pod.yaml

Then make the changes to the exported file using an editor (vi editor). Save the changes

vi my-new-pod.yaml

Then delete the existing pod

kubectl delete pod webapp

Then create a new pod with the edited file

kubectl create -f my-new-pod.yaml



Edit Deployments
With Deployments you can easily edit any field/property of the POD template. Since the pod template is a child of the deployment specification,  with every change the deployment will automatically delete and create a new pod with the new changes. So if you are asked to edit a property of a POD part of a deployment you may do that simply by running the command

kubectl edit deployment my-deployment



---------------------------------
 kubectl create secret generic db-secret \
> --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123
secret/db-secret created

apiVersion: v1
kind: Secret
metadata:
  creationTimestamp: "2023-12-04T08:19:40Z"
  name: db-secret
  namespace: default
  resourceVersion: "1077"
  uid: de447a8d-4800-48cf-b1f1-0ec0b1de2eee
data:
  DB_Host: c3FsMDE=
  DB_Password: cGFzc3dvcmQxMjM=
  DB_User: cm9vdA==
type: Opaque


========================================================

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2023-12-04T08:06:24Z"
  labels:
    name: webapp-pod
  name: webapp-pod
  namespace: default
  resourceVersion: "833"
  uid: d605956d-af88-4a84-85c2-e0a4b4a0a138
spec:
  containers:
  - image: kodekloud/simple-webapp-mysql
    imagePullPolicy: Always
    name: webapp
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    envFrom:
     - secretRef:
        name: db-secret
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-9qm4m
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: controlplane
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-9qm4m
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace

  ==================
  Service account creationT

      1  kubectl get serviceaccounts 
    2* kubectl describ
    3  kubectl get serviceaccounts 
    4  kubectl get serviceaccounts default 
    5  kubectl describe serviceaccounts default 
    6  kubectl get pods
    7  kubectl describe pods web-dashboard-97c9c59f6-6pvb8 | grep -i image
    8  kubectl  get pods web-dashboard-97c9c59f6-6pvb8 -o yaml > pod-web.yaml
    9  vi pod-web.yaml 
   10  kubectl create serviceaccount dashboard-sa
   11  ls -ll /var/rbac
   12  vi /var/rbac/dashboard-sa-role-binding.yaml 
   13  kubectl create token dashboard-sa 
   14  vi pod-web.yaml 
   15  kubectl edit deployments web-dashboard 
   16  kubectl --help
   17  kubectl edit pods web-dashboard-598c6cb6d-bbfsz 
   18  history

   ==================================
resources
-------------
limitation 
request value for pods
   controlplane ~ ➜  history
    1  kubectl get pods
    2  kubectl --help
    3  kubectl describe pods  rabbit 
    4  cat /proc/cpuinfo 
    5  kubectl delete pod rabbit 
    6  k get pods
    7  k get pods
    8  kubectl describe pod elephant 
    9  kubectl get pods elephant -o yaml > elphant.yaml
   10  vi elphant.yaml 
   11  kubectl replace --force -f elphant.yaml 
   12  k get  pods
   13  k delete pd
   14  k delete pod elephant
   15  k get  pods
   16  history

============================
Tain and tolerations:
---------------------
kubectl get nodes
kubectl describe node node01 | grep -i taint
kubectl taint node node01 spray=mortein:NoSchedule --> creating taint in node01 
kubectl run mosquitio --image=nginx
kubectl describe pod mosquito 
controlplane ~ ➜  history
    1  kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-
    2  k get pods
    3  k get pod -o wide
    4  history

--------------------------

Node selector and Node Affinity

controlplane ~ ➜  history
    1  k get nodes node01 -o wide
    2  k describe nodes node01 
    3  k describe nodes node01  | grep -i label
    4  k describe nodes node01 
    5  k edit node node01 
    6  kubectl --help
    7  kubectl label --help
    8  kubectl label node node01 color=blue
    9  k edit node node01 
   10  k run --help
   11  kubectl create deployment --help
   12  kubectl create deployment blue --image=nginx --replicas=3
   13  kubectl get deployments
   14  kubectl get pods -o wide
   15  kubectl get deployments -o wide > deploy.yaml
   16  vi deploy.yaml 
   17  kubectl edit deployment blue
   18  kubectl get pods -o wide
   19  history
   24  kubectl create deployment red --image=nginx --replicas=2 --dry-run=client -o yaml > dep.yaml
   25  vi dep.yaml 
   26  cat dep.yaml 
   27  kubectl create deployment -f dep.yaml 
   28  kubectl create -f dep.yaml 
   29  kubectl get deploy
   30  kubectl get pods -o wide

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: red
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      app: red
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: red
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
status: {}

=-------------------------------------------------
https://github.com/lucassha/CKAD-resources/tree/master
https://www.linkedin.com/pulse/my-ckad-exam-experience-atharva-chauthaiwale/


-------------------------
multi pods

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: yellow
  name: yellow
spec:
  containers:
  - image: busybox
    name: lemon
    command: ["sleep", "1000"]
    resources: {}
  - image: redis 
    name: gold
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

kubectl exec -i -t my-pod --container main-app -- /bin/bash

-----------------------------

init container
1  k get pods
    2  k describe pods green | grep -i init
    3  k describe pods red | grep -i init
    4  k describe pods blue | grep -i init
    5  k describe pods blue | grep -i image
    6  k describe pods blue
    7  k get pod blue
    8  k describe pods blue
    9  k describe pods blue | grep -i state
   10  k describe pods purple | grep -i state
   11  k describe pods purple | grep -i init
   12  k describe pods purple | grep -i init
   13  k get pod purp
   14  k describe pods purple | grep -i state
   15  k get pods
   16  k edit pod purple 
   17  k get pod red -o yaml 
   18  k get pod red -o yaml > red.yaml
   19  vi red.yaml 
   20  k apply --force -f red.yaml 
   25  k get pods --help
   26  k logs pods orange
   27  k logs pod orange
   28  k --help
   29  k describe pod orange 
   30  k edit pods orange 
   31  k replace --force -f /tmp/kubectl-edit-2855754534.yaml 
   32  k get pods
   33  k get pods
   34  history

----------------------------------------

readiness probe and livereadiness probe

ontrolplane ~ ➜  cat simple.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2023-12-10T11:09:37Z"
  labels:
    name: simple-webapp
  name: simple-webapp-2
  namespace: default
  resourceVersion: "867"
  uid: 2b937af0-80ff-43c5-a101-140fae821b6f
spec:
  containers:
  - env:
    - name: APP_START_DELAY
      value: "80"
    image: kodekloud/webapp-delayed-start
    imagePullPolicy: Always
    name: simple-webapp
    ports:
    - containerPort: 8080
      protocol: TCP
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-d2nl8
      readOnly: true
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
    livenessProbe:
      httpGet:
        path: /live
        port: 8080
      periodSeconds: 1
      initialDelaySeconds: 80
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: controlplane
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-d2nl8
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace

for i in {1..20}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/ready 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""
done

--------

logs and monitoring

   1  k get pods
    2  k logs -f webapp-1 
    3  k get pods
    4  k logs -f webapp-2
    5  k describe pods webapp-2 
    6  k logs -f webapp-2 -c simple-webapp

    3  ls -ll
    4  cd kubernetes-metrics-server/
    5  ls -ll
    6  kubectl create -f .
    7  k get pods
    8  k get depoly
    9  k get deploy
   10  kubectl top node
   11  kubectl top pods
   12  kubectl top node

------------------------
Check the label, selector,annotations

   1  kubectl version --show
    2  kubectl version 
    3  kubectl version --help
    4  kubectl version --short
   12  k get pods -l env=dev | grep -iv 'Name' | wc -l
   13  k get pods -l bu=finance | grep -iv 'Name' | wc -l
   18  k get pods all -l env=prod,bu=finance,tier=frontend
   19  k get pods -l env=prod,bu=finance,tier=frontend
   20  k edit pods app-1-zzxdf
   21  vi replicaset-definition-1.yaml 
   22  vi replicaset-definition-1.yaml 
   23  k get replica
   24  k get replicasets.apps 
   25  k create -f replicaset-definition-1.yaml 
   26  k get replicasets.apps 
   32  k get replicasets.apps --show-label
   38  k get rs -o wide
   39  history
   40  history

------------------------

rolling update and recreate strategy

   1  k get deploy
    2  k get pods
    3  k get all
    4  cat curl-
    5  cat curl-test.sh 
    6  ./curl-test.sh 
    7  k get pods
    8  kubectl describe deployment | grep -i replicas
    9  kubectl describe deployment | grep -i images
   10  kubectl describe deployment | grep -i image
   11  kubectl describe deployment | grep -i strategy
   12  k get deployments.apps 
   13  k edit deployment frontend
   14  ls -ll
   15  k get deployments.apps frontend  -o yaml > dep.yaml
   16  vi dep.yaml 
   17  k apply -f dep.yaml 
   18  k get pods
   19  k rollout status deployment frontend 
   20  k get pods
   21  k rollout status deployment frontend 
   22  k rollout  history deployment frontend 
   28  vi dep.yaml 
   29  k apply -f dep.yaml 
   30  vi dep.yaml 
   31  k get deployments.apps frontend  -o yaml > dep.yaml
   32  vi dep.yaml 
   33  k apply -f dep.yaml 
   34  vi dep.yaml 
   35  k apply -f dep.yaml 
   36  k rollout status deployment frontend 
   37  k get pods
   38  k describe deployments.apps frontend 
   39  k edit deployments.apps frontend 
   40  k get pods
   41  k get pods
   42  ./curl-test.sh 
   43  ./curl-test.sh 
   44  k get pods
   45  history
   46 k rollout undo deployment frontend --to-revision=2 

   ------------------------
   canary deployment

      1  k get pods
    2  k get all
    3  k describe deployments.apps frontend | grep -i strategy
    4* kubectl describe svc frontend
    6  k get deployments.apps 
    7  k get svc
    8  k edit svc frontend-service 
    9  k edit deployments.apps frontend-v2 
   10  k get pods
   11  k edit deployments.apps frontend-v2 
   12  k get pods
   13  k edit deploy frontend
   14  k edit deploy frontend-v2 
   15  k get pods
   16  k get deploy
   17  k delete deploy frontend
   18  k get deploy
       k scale depoly --replicas=1 <depolymnet name>
   19  history

------------------------------
job and cron 

controlplane ~ ➜  cat throw-dice-pod-cron.yaml 
apiVersion: batch/v1
kind: CronJob
metadata:
  name: throw-dice-cron-job
spec:
  schedule: "30 21 * * *"
  jobTemplate:
    spec:
      completions: 3
      parallelism: 3
      backoffLimit: 25
      template:
        spec:
          containers:
          - name: throw-dice
            image: kodekloud/throw-dice
          restartPolicy: Never
------------
controlplane ~ ➜  cat job-pod.yaml 
apiVersion: batch/v1
kind: Job
metadata: 
   name: throw-dice-job


spec:
  backofflimit: 15
  templates:
    spec:
      containers:
        - name: throw-dice
          image: kodekloud/throw-dice
        restartPolicy: Never

  1  vi throw-dice-pod.yaml 
    2  k create -f throw-dice-pod.yaml 
    3  k get pos
    4  k get po
    5  k describe pods  throw-dice-pod 
    6  k logs throw-dice-pod 
    7  k logs pods throw-dice-pod 
    8  k logs pod throw-dice-pod 
    9  k logs throw-dice-pod 
   10  vi job-pod.yaml
   11  cat job-pod.yaml 
   12  k create -f job-pod.yaml 
   13  vi job-pod.yaml 
   14  k create -f job-pod.yaml 
   15  vi job-pod.yaml 
   16  k create -f job-pod.yaml 
   17  vi job-pod.yaml 
   18  k create -f job-pod.yaml 
   19  k get pods
   20  k logs throw-dice-pod 
   21  k get pods
   22  k logs throw-dice-job-hq8sk
   23  k describe jobs.batch throw-dice-job-hq8sk
   24  k describe pods throw-dice-job-hq8sk
   25  k get jobs
   26  k describe job throw-dice-job 
   27  vi job-pod.yaml 
   28  k delete jobs throw-dice-job 
   29  k create -f job-pod.yaml 
   30  k get pods
   31  watch k get pods
   32  watch kubectl get pods
   33  k describe jobs.batch throw-dice-job 
   34  vi job-pod.yaml 
   35  k delete jobs.batch throw-dice-job 
   36  k create -f job-pod.yaml 
   37  vi job-pod.yaml 
   38  k create -f job-pod.yaml 
   39  watch kubectl get pods
   40  vi throw-dice-pod-cron.yaml
   41  k create -f throw-dice-pod-cron.yaml 
   47  k create -f throw-dice-pod-cron.yaml 
   49  k get cronjobs.batch throw-dice-cron-job 
   50  cat throw-dice-pod-cron.yaml 
   51  history

   -------------------------
   service detailed

apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  selector:
    name: simple-webapp
  type: NodePort
  ports:
    - port: 8080
      targetPort: 8080
      nodePort: 30080

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2023-12-12T02:56:28Z"
  labels:
    component: apiserver
    provider: kubernetes
  name: kubernetes
  namespace: default
  resourceVersion: "191"
  uid: 5d4b6917-77e3-4667-9d73-0fbb83393013
spec:
  clusterIP: 10.43.0.1
  clusterIPs:
  - 10.43.0.1
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: https
    port: 443
    protocol: TCP
    targetPort: 6443
  sessionAffinity: None
  type: 
  
------------------------------------------
Ingress object

controlplane ~ ➜  cat ingress-pay-def.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path:  /pay
        pathType: Prefix
        backend:
          service:
            name: pay-service
            port:
              number: 8282

    1  k get pod -A
    2  k get svc -n app-space 
    3  k create namespace ingress-nginx
    4  k get namespaces 
    5  k create configmap -n ingress-nginx ingress-nginx-controller
    6  k get configmaps 
    7  k get configmaps  -A
    8  s="ingress-nginx-admission
ingress-nginx"
    9  for i in `echo $s`; do k create serviceaccount -n ingress-nginx $i; done
   10  k get roles -n ingress-nginx 
   11  k get roles 
   12  k get clusterrole
   13  k get roles -n ingress-nginx 
   14  k describe role ingress-nginx -n ingress-nginx 
   15  k describe role ingress-nginx-admission -n ingress-nginx 
   16  k get rolebindings.rbac.authorization.k8s.io 
   17  k get rolebindings.rbac.authorization.k8s.io -n ingress-nginx 
   18  k describe rolebindings.rbac.authorization.k8s.io -n ingress-nginx
   19  vi ingress-controller.yaml 
   20  k create -f ingress-controller.yaml 
   21  k get pod -n ingress-nginx 
   22  k get svc -n ingress-nginx 
   23  vi ingress-svc.yaml


   ontrolplane ~ ➜  cat ingress-controller.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
        - --election-id=ingress-controller-leader
        - --watch-ingress-without-class=true
        - --default-backend-service=app-space/default-http-backend
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: registry.k8s.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          runAsUser: 101
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission

---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30080
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: NodePort

---------------------
Network polices

ontrolplane ~ ➜  cat network-policy.yaml 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
    - Egress
  egress:
    - to:
      - podSelector:
          matchLabels:
            name: mysql
      ports:
        - protocol: TCP
          port: 3306

    - to:
      - podSelector:
          matchLabels:
            name: payroll
      ports:
        - protocol: TCP
          port: 8080


    1  k get networkpoliy
    2  k get networkpolicies.networking.k8s.io 
    3  k describe networkpolicies.networking.k8s.io payroll-policy 
    4  k get pods
    5  k get svc
    6  vi network-policy.yaml
    7  cat network-policy.yaml 
    8  k create -f  network-policy.yaml 
    9  k get networkpolicies.networking.k8s.io 
   10  k describe networkpolicies.networking.k8s.io internal-
   
   --------------------
   pv and pvc 

   1  ls -ll
    2  vi webap.yaml 
    3  k replace --force -f webap.yaml 
    4  vi webap.yaml 
    5  k get pvc
    6  k replace --force -f webap.yaml 
    7  k get pvc
    8  vi webap.yaml 
    9  k replace --force -f webap.yaml 
   10  vi webap.yaml 
   11  k replace --force -f webap.yaml 
   12  k get pvc
   13  ls -ll /pv/log/
   14  du -sh /pv/log/
   15  k delete pvc claim-log-1 
   16  k get pvc
   17  k get pv
   18  history

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2023-12-15T05:59:28Z"
  name: webapp
  namespace: default
  resourceVersion: "601"
  uid: 65718951-1e9f-4dfb-8fd7-7c4e6c606291
spec:
  containers:
  - env:
    - name: LOG_HANDLERS
      value: file
    image: kodekloud/event-simulator
    imagePullPolicy: Always
    name: event-simulator
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /log
      name: log-volume
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-r2zjg
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: controlplane
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: log-volume
    persistentVolumeClaim:
      # directory location on host
      claimName: claim-log-1
  - name: kube-api-access-r2zjg
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace


controlplane /pv/log ➜  cat presistent-claim.yaml 
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Mi          


controlplane /pv/log ➜  cat presitent-volume-creation.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  capacity:
    storage: 100Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /pv/log

----------------------

authorization

    1  find . -type f -iname "config"
    8  kubectl get node
    9  kube-apiserver --help
   10  kubectl get namesapce
   11  kubectl get namespace
   12  k get pods -n kube-system 
   16  k describe pod kube-apiserver-controlplane -n kube-system | grep -i authorization
   22  k get roles.rbac.authorization.k8s.io -A --no-headers | wc -l
   23  kubectl describe role kube-proxy -n kube-system 
   24  kubectl get rolebindings.rbac.authorization.k8s.io
   25  kubectl get rolebindings.rbac.authorization.k8s.io -A
   26  kubectl get rolebindings.rbac.authorization.k8s.io -A | grep -i proxy
   27  kubectl describe rolebinding kube-proxy -n kube-system
   28  kubectl auth can-i --help
   29  kubectl auth can-i get pod --as dev-user
   32  kubectl auth can-i get pod --as dev-user -n default
   33  kubectl get pod --as dev-user
   34  kubectl get pod --as dev-user -n default
   36  k create role developer -o yaml > devloper.yaml
   37  kubectl create role developer --namespace=default --verb=list,create,delete --resource=pod --dry-run=client -o yaml > devlop.yaml
   40  kubectl create rolebinding dev-user-binding -n default --role=developer --user=dev-user --dry-run=client -o yaml > dev-role.yaml
   42  k create -f devlop.yaml 
   43  k create -f dev-role.yaml 
   44  k get role
   45  k get roles
   46  k get rolebindings.rbac.authorization.k8s.io 
   47  k describe rolebindings.rbac.authorization.k8s.io dev-user-binding 
   53  k describe pod dark-blue-app -n blue --as dev-user
   54  k get role -n blue
   55  k get rolebindings.rbac.authorization.k8s.io -n blue
   56  k describe rolebindings.rbac.authorization.k8s.io dev-user-binding -n blue
   57  k describe role -n blue
   58  k get pod dark-blue-app --as dev-user
   59  k get pod dark-blue-app
   60  k get pod dark-blue-app -n blue
   61  k get pod dark-blue-app -n blue --as dev-user
   62  k edit role developer -n blue 
   63  k describe role -n blue
   64  k get pod dark-blue-app -n blue --as dev-user
   65  k get namespace
   66  k edit role developer -n blue 
   67  k get pod dark-blue-app -n blue --as dev-user
   68  k edit role developer -n blue 


apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: "2023-12-19T05:55:20Z"
  name: developer
  namespace: blue
  resourceVersion: "5068"
  uid: f653bae7-8b88-466e-9e6d-00020d00cd05
rules:
- apiGroups:
  - ""
  resourceNames:
  - dark-blue-app  [pod name]
  resources:
  - pods
  verbs:
  - get
  - watch
  - create
  - delete
- apiGroups:
  - apps
  resources:
  - deployments
  verbs:
  - create

  =============================

  cluster role based method

     1  kubectl get nodes
    2  kubectl get clusterrole
    3  kubectl get clusterrolebindings.rbac.authorization.k8s.io 
    4  kubectl get clusterrole --no-headers | wc -l
    5  kubectl get clusterrolebindings.rbac.authorization.k8s.io --no-headers | wc -l
    6  kubectl get namespaces 
    7  kubectl get clusterrole cluster-admin 
    8  kubectl describe clusterrole cluster-admin 
    9  kubectl api-resources --namespaced=false
   11  kubectl describe clusterrolebindings.rbac.authorization.k8s.io cluster-admin
   12  kubectl create clusterrole --help
   13  kubectl create clusterrole --verb=get,list,watch --resource=node --dry-run =client -o yaml > node-clusterrole.yaml
   14  kubectl create clusterrole --verb=get,list,watch --resource=node --dry-run=client -o yaml > node-clusterrole.yaml
   15  kubectl create clusterrole node-cluster-rule --verb=get,list,watch --resource=node --dry-run=client -o yaml > node-clusterrole.yaml
   16  vi node-clusterrole.yaml 
   17  kubectl auth can-i list nodes --as michelle
   18  kubectl create clusterrole node-cluster-rule --verb=get,list,watch,update,delete --resource=node --dry-run=client -o yaml > node-clusterrole.yaml
   21  kubectl create clusterrolebinding node-cluster-binding --clusterrole=node-cluster-rule --user=michelle --dry-run=client -o yaml > node-cluster-bind.yaml
   22  vi node-cluster-bind.yaml 
   23  k create -f node-clusterrole.yaml 
   24  k create -f node-cluster-bind.yaml 
   25  kubectl auth can-i list nodes --as michelle
   26  vi node-clusterrole.yaml 
   27  k edit clusterrole node-cluster-rule
   28  k auth can-i list storageclass --as michelle
   29  k auth can-i list pv --as michelle
   30  k auth can-i list sc --as michelle
   31  k edit clusterrole node-cluster-rule
   32  vi node-clusterrole.yaml 
   33  k create clusterrole storage-admin --verb=get,list,create,delete,watch --resource=persistentvolumes,storageclasses --dry-run=client -o yaml > storage-admin.yaml
   37  k create clusterrolebinding michelle-storage-admin --clusterrole=storage-admin --user=michelle --dry-run=client -o yaml > storagebinding.yaml
   38  vi storagebinding.yaml 

controlplane ~ ➜  cat storage-admin.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  creationTimestamp: null
  name: storage-admin
rules:
- apiGroups:
  - ""
  resources:
  - persistentvolumes
  verbs:
  - get
  - list
  - create
  - delete
  - watch
- apiGroups:
  - storage.k8s.io
  resources:
  - storageclasses
  verbs:
  - get
  - list
  - create
  - delete
  - watch


  controlplane ~ ➜  cat storagebinding.yaml 
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  creationTimestamp: null
  name: michelle-storage-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: storage-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: michelle

-----------------------------------------
  Admission Controller

controlplane ~ ➜  history
    1  kubectl exec -it kube-apiserver-controlplane -n kube-system -- /bin/bash
    2  kubectl exec -it kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep -i admission
    3  kubectl exec -it kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep -i 'enable-admission-plugins'
    4  kubectl exec -it kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep -i 'enable-admission-plugins'| grep -iw 'NamespaceAutoprovision'
    5  vi /etc/kubernetes/manifests/kube-apiserver.yaml
    6  kubectl run nginx --image nginx -n blue
    7  k get pods
    8  k get namespaces 
    9  k get pods -n kube-system 
   10  k edit kube-apiserver-controlplane -n kube-system 
   11  k edit pod kube-apiserver-controlplane -n kube-system 
   12  k replace --force -f /tmp/kubectl-edit-3529339972.yaml
   13  k edit pod kube-apiserver-controlplane -n kube-system 
   14  vi /etc/kubernetes/manifests/kube-apiserver.yaml
   15  k get pods -n kube-system 
   16  kubectl run nginx --image nginx -n blue
   17  kubectl get namespace
   18  k get pods -n blue 
   19  vi /etc/kubernetes/manifests/kube-apiserver.yaml 
   20  vi /etc/kubernetes/manifests/etcd.yaml 
   21  k get pod -n kube-system
   22  ps -ef | grep kube-apiserver | grep admission-plugins
   23  ps -ef | grep kube-apiserver | grep disable-admission-plugins
   24  history

  ----------------
  API versions

  apis under lot of groups available for each API group ahave separate version 
  V1 - GA global stable group
  V1Alpha1
  V1 Beta1

 1  kubectl api-resources 
    2  kubectl api-resources | grep -iE "deploy|replicasets,cronjobs,customersourcedef"
    3  kubectl api-resources | grep -iE "deploy|replicasets|cronjobs|customersourcedef"
    4  kubectl api-resources | grep -iE "deploy|replicasets|cronjobs|customersour"
    5  kubectl api-resources | grep -iE "deploy|replicasets|cronjobs|customresourcedefinitions"
    6  kubectl version --short
    7  kubectl explain deploy
    8  kubectl explain job
    9  kubectl explain authorization
   10  kubectl proxy 8001&
   11  curl localhost:8001/apis/authorization.k8s.io
   12  curl localhost:8001/apis/rbac.authorization.k8s.io
   13  vi /etc/kubernetes/manifests/kube-apiserver.yaml 
   14  cp /etc/kubernetes/manifests/kube-apiserver.yaml /etc/kubernetes/manifests/kube-apiserver.yaml-backup`date +%F`
   15  vi /etc/kubernetes/manifests/kube-apiserver.yaml 
   16  k get pods -n kube-system
   17  k get pods
   18  k get nodes
   19  k get pods -n kube-system
   20  curl localhost:8001/apis/rbac.authorization.k8s.io
   21  kubectl get po -n kube-system
   22  curl localhost:8001/apis/rbac.authorization.k8s.io
   23  kubectl get po -n kube-system
   24  curl localhost:8001/apis/rbac.authorization.k8s.io
   25  curl localhost:8001/apis/rbac.authorization.k8s.io/v1alpha1
   26  curl localhost:8001/apis/rbac.authorization.k8s.io/v1
   27  curl localhost:8001/apis/rbac.authorization.k8s.io/v1alpha1
   28  vi /etc/kubernetes/manifests/kube-apiserver.yaml 
   29  ls -ll
   30  sudo install -o root -g root -m 0755 kubectl-convert /usr/local/bin/kubectl-convert
   31  kubectl-convert --version
   32  vi ingress-old.yaml 
   33  curl localhost:8001/apis/networking.k8s.io/v1
   34  curl localhost:8001/apis/networking.k8s.io/
   35  vi ingress-old.yaml 
   36  kubectl-convert --help
   37  kubectl convert -f ingress-old.yaml --outpu-version networking.k8s.io/v1 | kubectl create -f -
   38  vi ingress-old.yaml 
   39  kubectl convert -f ingress-old.yaml -o yaml
   40  history

-------------
customer resource definition

controlplane ~ ➜  history
    1  ls -ll
    2  vi crd.yaml 
    3  k create -f  crd.yaml 
    4  k describe customresourcedefinitions.apiextensions.k8s.io internals.datasets.kodekloud.com 
    5  vi crd.yaml 
    6  ls -ll
    7  k create -f  custom.yaml 
    8  vi crd.yaml 
    9  vi custom.yaml 
   10  k create -f  custom.yaml 
   11  vi crd.yaml 
   12  vi custom.yaml 
   13  vi crd.yaml 
   14  k apply --force -f crd.yaml 
   15  k describe customresourcedefinitions.apiextensions.k8s.io internals.datasets.kodekloud.com 
   16  k create -f  custom.yaml 
   17  k describe customresourcedefinitions.apiextensions.k8s.io collectors.monitoring.controller 
   18  vi datacenter.yaml
   19  k create -f datacenter.yaml 
   20  k describe customresourcedefinitions.apiextensions.k8s.io globals.traffic.controller 
   21  k describe customresourcedefinitions.apiextensions.k8s.io globals.traffic.controller | grep -iA "short"
   22  k describe customresourcedefinitions.apiextensions.k8s.io globals.traffic.controller | grep -iA2 "short"
   23  history

controlplane ~ ➜  cat crd.yaml 
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name:  internals.datasets.kodekloud.com
spec:
  group: datasets.kodekloud.com
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                internalLoad:
                  type: string
                range:
                  type: integer
                percentage:
                  type: string
  scope: Namespaced
  names: 
    plural: internals
    singular: internal
    kind: Internal
    shortNames:
    - int

controlplane ~ ➜  cat custom.yaml 
---
kind: Internal
apiVersion: datasets.kodekloud.com/v1
metadata:
  name: internal-space
  namespace: default
spec:
  internalLoad: "high"
  range: 80
  percentage: "50"

---------------------

controlplane ~ ➜  history
    1  cat /etc/os-release 
    2  uname -ar
    3  curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
    4  ls -ll
    5  sudo apt-get install apt-transport-https --yes
    6  echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
    7  sudo apt-get update
    8  sudo apt-get install helm
    9  helm --help
   14  helm --help | grep -i environment
   15  helm env
   16  helm version
   17  helm --help | grep -i verbose
   18  history