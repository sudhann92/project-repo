kubectl run redis --image=redis --namespace=finance --dry-run=client -o yaml > pod-finanace.yaml
kubectl get pods --all-namespaces

While you would be working mostly the declarative way - using definition files, imperative commands can help in getting one-time tasks done quickly, as well as generate a definition template easily. This would help save a considerable amount of time during your exams.

Before we begin, familiarize yourself with the two options that can come in handy while working with the below commands:

--dry-run: By default, as soon as the command is run, the resource will be created. If you simply want to test your command, use the --dry-run=client option. This will not create the resource. Instead, tell you whether the resource can be created and if your command is right.

-o yaml: This will output the resource definition in YAML format on the screen.



Use the above two in combination along with Linux output redirection to generate a resource definition file quickly, that you can then modify and create resources as required, instead of creating the files from scratch.



kubectl run nginx --image=nginx --dry-run=client -o yaml > nginx-pod.yaml



POD
Create an NGINX Pod

kubectl run nginx --image=nginx



Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)

kubectl run nginx --image=nginx --dry-run=client -o yaml



Deployment
Create a deployment

kubectl create deployment --image=nginx nginx



Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)

kubectl create deployment --image=nginx nginx --dry-run -o yaml



Generate Deployment with 4 Replicas

kubectl create deployment nginx --image=nginx --replicas=4



You can also scale deployment using the kubectl scale command.

kubectl scale deployment nginx --replicas=4



Another way to do this is to save the YAML definition to a file and modify

kubectl create deployment nginx --image=nginx--dry-run=client -o yaml > nginx-deployment.yaml



You can then update the YAML file with the replicas or any other field before creating the deployment.



Service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors)

Or

kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml (This will not use the pods' labels as selectors; instead it will assume selectors as app=redis. You cannot pass in selectors as an option. 
So it does not work well if your pod has a different label set. So generate the file and modify the selectors before creating the service)



Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes:

kubectl expose pod nginx --port=80 --name nginx-service --type=NodePort --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors, but you cannot specify the node port. You have to generate a definition file and then add the node port in manually before creating the service with the pod.)

Or

kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml

(This will not use the pods' labels as selectors)

Both the above commands have their own challenges. While one of it cannot accept a selector the other cannot accept a node port. I would recommend going with the `kubectl expose` command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service.



kubectl run nginx-pod --image=nginx:alpine


kubectl run redis --image=redis:alpine --dry-run=client -o yaml > pod.yaml

controlplane ~ ➜  vi pod.yaml 

controlplane ~ ➜  kubectl create -f pod.yaml 

Cluster IP 
kubectl expose pod redis --port=6379 --name redis-service

Create depolyment in command
kubectl create deployment --image=kodekloud/webapp-color webapp --replicas=3

create pod and expose port in 8080
kubectl run custom-nginx --image=nginx --port=8080

create namespace
kubectl create namespace dev-ns

create new deployment in dev-ns namespace
kubectl create deployment redis-deploy --namespace dev-ns --image=redis --replicas=2
deployment.apps/redis-deploy created


Create a pod called httpd using the image httpd:alpine in the default namespace. 
Next, create a service of type ClusterIP by the same name (httpd). The target port for the service should be 80
controlplane ~ ➜  kubectl run httpd --image=httpd:alpine --port=80 --expose
service/httpd created
pod/httpd created


A copy of the file with your changes is saved in a temporary location as shown above.

You can then delete the existing pod by running the command:

kubectl delete pod webapp



Then create a new pod with your changes using the temporary file

kubectl create -f /tmp/kubectl-edit-ccvrq.yaml



2. The second option is to extract the pod definition in YAML format to a file using the command

kubectl get pod webapp -o yaml > my-new-pod.yaml

Then make the changes to the exported file using an editor (vi editor). Save the changes

vi my-new-pod.yaml

Then delete the existing pod

kubectl delete pod webapp

Then create a new pod with the edited file

kubectl create -f my-new-pod.yaml



Edit Deployments
With Deployments you can easily edit any field/property of the POD template. Since the pod template is a child of the deployment specification,  with every change the deployment will automatically delete and create a new pod with the new changes. So if you are asked to edit a property of a POD part of a deployment you may do that simply by running the command

kubectl edit deployment my-deployment



---------------------------------
 kubectl create secret generic db-secret \
> --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123
secret/db-secret created

apiVersion: v1
kind: Secret
metadata:
  creationTimestamp: "2023-12-04T08:19:40Z"
  name: db-secret
  namespace: default
  resourceVersion: "1077"
  uid: de447a8d-4800-48cf-b1f1-0ec0b1de2eee
data:
  DB_Host: c3FsMDE=
  DB_Password: cGFzc3dvcmQxMjM=
  DB_User: cm9vdA==
type: Opaque


========================================================

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2023-12-04T08:06:24Z"
  labels:
    name: webapp-pod
  name: webapp-pod
  namespace: default
  resourceVersion: "833"
  uid: d605956d-af88-4a84-85c2-e0a4b4a0a138
spec:
  containers:
  - image: kodekloud/simple-webapp-mysql
    imagePullPolicy: Always
    name: webapp
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    envFrom:
     - secretRef:
        name: db-secret
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-9qm4m
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: controlplane
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-9qm4m
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2023-12-04T08:06:24Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2023-12-04T08:06:34Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2023-12-04T08:06:34Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2023-12-04T08:06:24Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: containerd://8c5864f9845ad3d531ef20ba3370e51ac9d760fb97cda450a2a82a79cd5b0d07
    image: docker.io/kodekloud/simple-webapp-mysql:latest
    imageID: docker.io/kodekloud/simple-webapp-mysql@sha256:92943d2b3ea4a1db7c8a9529cd5786ae3b9999e0246ab665c29922e9800d1b41
    lastState: {}
    name: webapp
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2023-12-04T08:06:33Z"
  hostIP: 192.13.38.6
  phase: Running
  podIP: 10.42.0.9
  podIPs:
  - ip: 10.42.0.9
  qosClass: BestEffort
  startTime: "2023-12-04T08:06:24Z"

  ==================
  Service account creationT

      1  kubectl get serviceaccounts 
    2* kubectl describ
    3  kubectl get serviceaccounts 
    4  kubectl get serviceaccounts default 
    5  kubectl describe serviceaccounts default 
    6  kubectl get pods
    7  kubectl describe pods web-dashboard-97c9c59f6-6pvb8 | grep -i image
    8  kubectl  get pods web-dashboard-97c9c59f6-6pvb8 -o yaml > pod-web.yaml
    9  vi pod-web.yaml 
   10  kubectl create serviceaccount dashboard-sa
   11  ls -ll /var/rbac
   12  vi /var/rbac/dashboard-sa-role-binding.yaml 
   13  kubectl create token dashboard-sa 
   14  vi pod-web.yaml 
   15  kubectl edit deployments web-dashboard 
   16  kubectl --help
   17  kubectl edit pods web-dashboard-598c6cb6d-bbfsz 
   18  history

   ==================================
resources
-------------
limitation 
request value for pods
   controlplane ~ ➜  history
    1  kubectl get pods
    2  kubectl --help
    3  kubectl describe pods  rabbit 
    4  cat /proc/cpuinfo 
    5  kubectl delete pod rabbit 
    6  k get pods
    7  k get pods
    8  kubectl describe pod elephant 
    9  kubectl get pods elephant -o yaml > elphant.yaml
   10  vi elphant.yaml 
   11  kubectl replace --force -f elphant.yaml 
   12  k get  pods
   13  k delete pd
   14  k delete pod elephant
   15  k get  pods
   16  history

============================
Tain and tolerations:
---------------------
kubectl get nodes
kubectl describe node node01 | grep -i taint
kubectl taint node node01 spray=mortein:NoSchedule --> creating taint in node01 
kubectl run mosquitio --image=nginx
kubectl describe pod mosquito 
controlplane ~ ➜  history
    1  kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-
    2  k get pods
    3  k get pod -o wide
    4  history

--------------------------

Node selector and Node Affinity

controlplane ~ ➜  history
    1  k get nodes node01 -o wide
    2  k describe nodes node01 
    3  k describe nodes node01  | grep -i label
    4  k describe nodes node01 
    5  k edit node node01 
    6  kubectl --help
    7  kubectl label --help
    8  kubectl label node node01 color=blue
    9  k edit node node01 
   10  k run --help
   11  kubectl create deployment --help
   12  kubectl create deployment blue --image=nginx --replicas=3
   13  kubectl get deployments
   14  kubectl get pods -o wide
   15  kubectl get deployments -o wide > deploy.yaml
   16  vi deploy.yaml 
   17  kubectl edit deployment blue
   18  kubectl get pods -o wide
   19  history
   24  kubectl create deployment red --image=nginx --replicas=2 --dry-run=client -o yaml > dep.yaml
   25  vi dep.yaml 
   26  cat dep.yaml 
   27  kubectl create deployment -f dep.yaml 
   28  kubectl create -f dep.yaml 
   29  kubectl get deploy
   30  kubectl get pods -o wide

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: red
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      app: red
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: red
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
status: {}

=-------------------------------------------------
https://github.com/lucassha/CKAD-resources/tree/master
https://www.linkedin.com/pulse/my-ckad-exam-experience-atharva-chauthaiwale/


-------------------------
multi pods

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: yellow
  name: yellow
spec:
  containers:
  - image: busybox
    name: lemon
    command: ["sleep", "1000"]
    resources: {}
  - image: redis 
    name: gold
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

kubectl exec -i -t my-pod --container main-app -- /bin/bash

-----------------------------

init container
1  k get pods
    2  k describe pods green | grep -i init
    3  k describe pods red | grep -i init
    4  k describe pods blue | grep -i init
    5  k describe pods blue | grep -i image
    6  k describe pods blue
    7  k get pod blue
    8  k describe pods blue
    9  k describe pods blue | grep -i state
   10  k describe pods purple | grep -i state
   11  k describe pods purple | grep -i init
   12  k describe pods purple | grep -i init
   13  k get pod purp
   14  k describe pods purple | grep -i state
   15  k get pods
   16  k edit pod purple 
   17  k get pod red -o yaml 
   18  k get pod red -o yaml > red.yaml
   19  vi red.yaml 
   20  k apply --force -f red.yaml 
   25  k get pods --help
   26  k logs pods orange
   27  k logs pod orange
   28  k --help
   29  k describe pod orange 
   30  k edit pods orange 
   31  k replace --force -f /tmp/kubectl-edit-2855754534.yaml 
   32  k get pods
   33  k get pods
   34  history

----------------------------------------

readiness probe and livereadiness probe

ontrolplane ~ ➜  cat simple.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2023-12-10T11:09:37Z"
  labels:
    name: simple-webapp
  name: simple-webapp-2
  namespace: default
  resourceVersion: "867"
  uid: 2b937af0-80ff-43c5-a101-140fae821b6f
spec:
  containers:
  - env:
    - name: APP_START_DELAY
      value: "80"
    image: kodekloud/webapp-delayed-start
    imagePullPolicy: Always
    name: simple-webapp
    ports:
    - containerPort: 8080
      protocol: TCP
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-d2nl8
      readOnly: true
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
    livenessProbe:
      httpGet:
        path: /live
        port: 8080
      periodSeconds: 1
      initialDelaySeconds: 80
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: controlplane
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-d2nl8
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace

for i in {1..20}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/ready 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""
done

--------

logs and monitoring

   1  k get pods
    2  k logs -f webapp-1 
    3  k get pods
    4  k logs -f webapp-2
    5  k describe pods webapp-2 
    6  k logs -f webapp-2 -c simple-webapp

    3  ls -ll
    4  cd kubernetes-metrics-server/
    5  ls -ll
    6  kubectl create -f .
    7  k get pods
    8  k get depoly
    9  k get deploy
   10  kubectl top node
   11  kubectl top pods
   12  kubectl top node

------------------------
Check the label, selector,annotations

   1  kubectl version --show
    2  kubectl version 
    3  kubectl version --help
    4  kubectl version --short
   12  k get pods -l env=dev | grep -iv 'Name' | wc -l
   13  k get pods -l bu=finance | grep -iv 'Name' | wc -l
   18  k get pods all -l env=prod,bu=finance,tier=frontend
   19  k get pods -l env=prod,bu=finance,tier=frontend
   20  k edit pods app-1-zzxdf
   21  vi replicaset-definition-1.yaml 
   22  vi replicaset-definition-1.yaml 
   23  k get replica
   24  k get replicasets.apps 
   25  k create -f replicaset-definition-1.yaml 
   26  k get replicasets.apps 
   32  k get replicasets.apps --show-label
   38  k get rs -o wide
   39  history
   40  history

------------------------

rolling update and recreate strategy

   1  k get deploy
    2  k get pods
    3  k get all
    4  cat curl-
    5  cat curl-test.sh 
    6  ./curl-test.sh 
    7  k get pods
    8  kubectl describe deployment | grep -i replicas
    9  kubectl describe deployment | grep -i images
   10  kubectl describe deployment | grep -i image
   11  kubectl describe deployment | grep -i strategy
   12  k get deployments.apps 
   13  k edit deployment frontend
   14  ls -ll
   15  k get deployments.apps frontend  -o yaml > dep.yaml
   16  vi dep.yaml 
   17  k apply -f dep.yaml 
   18  k get pods
   19  k rollout status deployment frontend 
   20  k get pods
   21  k rollout status deployment frontend 
   22  k rollout  history deployment frontend 
   28  vi dep.yaml 
   29  k apply -f dep.yaml 
   30  vi dep.yaml 
   31  k get deployments.apps frontend  -o yaml > dep.yaml
   32  vi dep.yaml 
   33  k apply -f dep.yaml 
   34  vi dep.yaml 
   35  k apply -f dep.yaml 
   36  k rollout status deployment frontend 
   37  k get pods
   38  k describe deployments.apps frontend 
   39  k edit deployments.apps frontend 
   40  k get pods
   41  k get pods
   42  ./curl-test.sh 
   43  ./curl-test.sh 
   44  k get pods
   45  history
   46 k rollout undo deployment frontend --to-revision=2 

   ------------------------
   canary deployment

      1  k get pods
    2  k get all
    3  k describe deployments.apps frontend | grep -i strategy
    4* kubectl describe svc frontend
    6  k get deployments.apps 
    7  k get svc
    8  k edit svc frontend-service 
    9  k edit deployments.apps frontend-v2 
   10  k get pods
   11  k edit deployments.apps frontend-v2 
   12  k get pods
   13  k edit deploy frontend
   14  k edit deploy frontend-v2 
   15  k get pods
   16  k get deploy
   17  k delete deploy frontend
   18  k get deploy
       k scale depoly --replicas=1 <depolymnet name>
   19  history

------------------------------
job and cron 

controlplane ~ ➜  cat throw-dice-pod-cron.yaml 
apiVersion: batch/v1
kind: CronJob
metadata:
  name: throw-dice-cron-job
spec:
  schedule: "30 21 * * *"
  jobTemplate:
    spec:
      completions: 3
      parallelism: 3
      backoffLimit: 25
      template:
        spec:
          containers:
          - name: throw-dice
            image: kodekloud/throw-dice
          restartPolicy: Never
------------
controlplane ~ ➜  cat job-pod.yaml 
apiVersion: batch/v1
kind: Job
metadata: 
   name: throw-dice-job


spec:
  backofflimit: 15
  templates:
    spec:
      containers:
        - name: throw-dice
          image: kodekloud/throw-dice
        restartPolicy: Never

  1  vi throw-dice-pod.yaml 
    2  k create -f throw-dice-pod.yaml 
    3  k get pos
    4  k get po
    5  k describe pods  throw-dice-pod 
    6  k logs throw-dice-pod 
    7  k logs pods throw-dice-pod 
    8  k logs pod throw-dice-pod 
    9  k logs throw-dice-pod 
   10  vi job-pod.yaml
   11  cat job-pod.yaml 
   12  k create -f job-pod.yaml 
   13  vi job-pod.yaml 
   14  k create -f job-pod.yaml 
   15  vi job-pod.yaml 
   16  k create -f job-pod.yaml 
   17  vi job-pod.yaml 
   18  k create -f job-pod.yaml 
   19  k get pods
   20  k logs throw-dice-pod 
   21  k get pods
   22  k logs throw-dice-job-hq8sk
   23  k describe jobs.batch throw-dice-job-hq8sk
   24  k describe pods throw-dice-job-hq8sk
   25  k get jobs
   26  k describe job throw-dice-job 
   27  vi job-pod.yaml 
   28  k delete jobs throw-dice-job 
   29  k create -f job-pod.yaml 
   30  k get pods
   31  watch k get pods
   32  watch kubectl get pods
   33  k describe jobs.batch throw-dice-job 
   34  vi job-pod.yaml 
   35  k delete jobs.batch throw-dice-job 
   36  k create -f job-pod.yaml 
   37  vi job-pod.yaml 
   38  k create -f job-pod.yaml 
   39  watch kubectl get pods
   40  vi throw-dice-pod-cron.yaml
   41  k create -f throw-dice-pod-cron.yaml 
   47  k create -f throw-dice-pod-cron.yaml 
   49  k get cronjobs.batch throw-dice-cron-job 
   50  cat throw-dice-pod-cron.yaml 
   51  history

   -------------------------
   service detailed

apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  selector:
    name: simple-webapp
  type: NodePort
  ports:
    - port: 8080
      targetPort: 8080
      nodePort: 30080

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2023-12-12T02:56:28Z"
  labels:
    component: apiserver
    provider: kubernetes
  name: kubernetes
  namespace: default
  resourceVersion: "191"
  uid: 5d4b6917-77e3-4667-9d73-0fbb83393013
spec:
  clusterIP: 10.43.0.1
  clusterIPs:
  - 10.43.0.1
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: https
    port: 443
    protocol: TCP
    targetPort: 6443
  sessionAffinity: None
  type: 
  
------------------------------------------
Ingress object

controlplane ~ ➜  cat ingress-pay-def.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path:  /pay
        pathType: Prefix
        backend:
          service:
            name: pay-service
            port:
              number: 8282

    1  k get pod -A
    2  k get svc -n app-space 
    3  k create namespace ingress-nginx
    4  k get namespaces 
    5  k create configmap -n ingress-nginx ingress-nginx-controller
    6  k get configmaps 
    7  k get configmaps  -A
    8  s="ingress-nginx-admission
ingress-nginx"
    9  for i in `echo $s`; do k create serviceaccount -n ingress-nginx $i; done
   10  k get roles -n ingress-nginx 
   11  k get roles 
   12  k get clusterrole
   13  k get roles -n ingress-nginx 
   14  k describe role ingress-nginx -n ingress-nginx 
   15  k describe role ingress-nginx-admission -n ingress-nginx 
   16  k get rolebindings.rbac.authorization.k8s.io 
   17  k get rolebindings.rbac.authorization.k8s.io -n ingress-nginx 
   18  k describe rolebindings.rbac.authorization.k8s.io -n ingress-nginx
   19  vi ingress-controller.yaml 
   20  k create -f ingress-controller.yaml 
   21  k get pod -n ingress-nginx 
   22  k get svc -n ingress-nginx 
   23  vi ingress-svc.yaml


   ontrolplane ~ ➜  cat ingress-controller.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
        - --election-id=ingress-controller-leader
        - --watch-ingress-without-class=true
        - --default-backend-service=app-space/default-http-backend
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: registry.k8s.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          runAsUser: 101
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission

---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30080
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: NodePort
