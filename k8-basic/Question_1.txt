
Create a Persistent Volume called log-volume. It should make use of a storage class name manual. It should use RWX as the access mode and have a size of 1Gi. The volume should use the hostPath /opt/volume/nginx

Next, create a PVC called log-claim requesting a minimum of 200Mi of storage. This PVC should bind to log-volume.

Mount this in a pod called logger at the location /var/www/nginx. This pod should use the image nginx:alpine



controlplane ~ ➜  cat log-volume.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: log-volume
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  storageClassName: manual
  hostPath :
    path: /opt/volume/nginx

controlplane ~ ➜  cat log-claim.yml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: log-claim
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 200Mi
  storageClassName: manual


controlplane ~ ➜  cat logger.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: logger
  name: logger
spec:
  containers:

  - image: nginx:alpine
    name: logger
    resources: {}

    volumeMounts:
    - name: logs
      mountPath: /var/www/nginx
  volumes:
  - name: logs
    persistentVolumeClaim: 
        claimName: log-claim
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}


------------------------------------------------------

We have deployed a new pod called secure-pod and a service called secure-service. Incoming or Outgoing connections to this pod are not working.
Troubleshoot why this is happening.

Make sure that incoming connection from the pod webapp-color are successful.

checking the default policy
controlplane ~ ✦2 ➜  k describe netpol default-deny 
Name:         default-deny
Namespace:    default
Created on:   2023-12-27 21:11:58 -0500 EST
Labels:       <none>
Annotations:  <none>
Spec:
  PodSelector:     <none> (Allowing the specific traffic to all pods in this namespace)
  Allowing ingress traffic:
    <none> (Selected pods are isolated for ingress connectivity)
  Not affecting egress traffic
  Policy Types: Ingress




controlplane ~ ✦2 ➜  cat netpolicay.yml 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  creationTimestamp: "2023-12-28T02:11:58Z"
  generation: 1
  name: network-policy
  namespace: default
  resourceVersion: "6582"
  uid: fe525f83-0ee5-4678-aa83-4b647c64b03e
spec:
  podSelector:
     matchLabels:
       run: secure-pod
  policyTypes:
  - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              name: webapp-color 
      ports:
        - protocol: TCP
          port: 80
status: {}

--------------------------------------------------------------------------

Create a pod called time-check in the dvl1987 namespace. This pod should run a container called time-check that uses the busybox image.
Create a config map called time-config with the data TIME_FREQ=10 in the same namespace.
The time-check container should run the command: while true; do date; sleep $TIME_FREQ;done and write the result to the location /opt/time/time-check.log.
The path /opt/time on the pod should mount a volume that lasts the lifetime of this pod.



controlplane ~ ✖ cat timecheck.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: time-check
  name: time-check
  namespace: dvl1987
spec:
  volumes:
  - name: log-volume
    emptyDir: {}  
  containers:
  - image: busybox
    name: time-check
    command: [ "/bin/sh", "-c", "while true; do date; sleep $TIME_FREQ;done > /opt/time/time-check.log" ]
    env:
        # Define the environment variable
        - name: TIME_FREQ
          valueFrom:
            configMapKeyRef:
              # The ConfigMap containing the value you want to assign to SPECIAL_LEVEL_KEY
              name: time-config
              # Specify the key associated with the value
              key: TIME_FREQ   
    volumeMounts:
    - name: log-volume
      mountPath: /opt/time         
    resources: {}

  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}


----------------------------------------------------------

Create a new deployment called nginx-deploy, with one single container called nginx, image nginx:1.16 and 4 replicas.
The deployment should use RollingUpdate strategy with maxSurge=1, and maxUnavailable=2.

Next upgrade the deployment to version 1.17.

Finally, once all pods are updated, undo the update and go back to the previous version.


controlplane ~ ➜  cat deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 4
  selector:
    matchLabels:
      app: nginx-deploy
  strategy:
     type: RollingUpdate
     rollingUpdate:
        maxUnavailable: 2
        maxSurge: 1
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx-deploy
    spec:
      containers:
      - image: nginx:1.16
        name: nginx
        resources: {}
status: {}

================================================================

Create a redis deployment with the following parameters:

Name of the deployment should be redis using the redis:alpine image. It should have exactly 1 replica.

The container should request for .2 CPU. It should use the label app=redis.

It should mount exactly 2 volumes.

a. An Empty directory volume called data at path /redis-master-data.

b. A configmap volume called redis-config at path /redis-master.

c. The container should expose the port 6379.


The configmap has already been created.

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: redis
  name: redis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: redis
    spec:
      volumes:
        - name: data
          emptyDir: {}      
        - name: redis-config
          configMap:
             name: redis-config
      containers:
      - image: redis:alpine
        name: redis
        volumeMounts:
         - name: data
           mountPath: /redis-master-data       
         - name: redis-config
           mountPath: /redis-master
        ports:
        - containerPort: 6379      
        resources: 
           requests:
             cpu: "0.2"
status: {}


Add a taint to the node node01 of the cluster. Use the specification below:


key: app_type, value: alpha and effect: NoSchedule

Create a pod called alpha, image: redis with toleration to node01.

k taint node node01 app_type=alpha:NoSchedule

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: alpha
  name: alpha
spec:
  containers:
  - image: redis
    name: alpha
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  tolerations:
    - key: "app_type"
      operator: "Equal"
      value: "alpha"
      effect: "NoSchedule"  
status: {}

======================================
Apply a label app_type=beta to node controlplane. Create a new deployment called beta-apps with image: nginx and replicas: 3. Set Node Affinity to the deployment to place the PODs on controlplane only.


NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution

controlplane has the correct labels?

Deployment beta-apps: NodeAffinity set to requiredDuringSchedulingIgnoredDuringExecution ?

Deployment beta-apps has correct Key for NodeAffinity?

Deployment beta-apps has correct Value for NodeAffinity?

Deployment beta-apps has pods running only on controlplane?

Deployment beta-apps has 3 pods running?

answer
k label nodes controlplane app_type=beta

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: beta-apps
  name: beta-apps
spec:
  replicas: 3
  selector:
    matchLabels:
      app: beta-apps
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: beta-apps
    spec:
      affinity:
         nodeAffinity:
           requiredDuringSchedulingIgnoredDuringExecution:
             nodeSelectorTerms:
             - matchExpressions:
               - key: app_type
                 operator: In
                 values:
                  - beta
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}

==========================

We have deployed a new pod called pod-with-rprobe. This Pod has an initial delay before it is Ready. Update the newly created pod pod-with-rprobe with a readinessProbe using the given spec


httpGet path: /ready

httpGet port: 8080

readinessProbe with the correct httpGet path?

readinessProbe with the correct httpGet port?

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2024-01-03T02:18:35Z"
  labels:
    name: pod-with-rprobe
  name: pod-with-rprobe
  namespace: default
  resourceVersion: "7804"
  uid: 86c04192-8fc8-4293-963f-302bf0961e65
spec:
  containers:
  - env:
    - name: APP_START_DELAY
      value: "180"
    image: kodekloud/webapp-delayed-start
    imagePullPolicy: Always
    name: pod-with-rprobe
    ports:
    - containerPort: 8080
      protocol: TCP
    readinessProbe:
      failureThreshold: 3
      httpGet:
        host: pod-with-rprobe
        path: /ready
        port: 8080
        scheme: HTTP
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1

      ----------------

Create a deployment called my-webapp with image: nginx, label tier:frontend and 2 replicas. Expose the deployment as a NodePort service with name front-end-service , port: 80 and NodePort: 30083


Deployment my-webapp created?

image: nginx

Replicas = 2 ?

service front-end-service created?

service Type created correctly?

Correct node Port used?

controlplane ~ ✖ cat my-webapp.yml 
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: my-webapp
    tier: rontend
  name: my-webapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: my-webapp
      tier: frontend
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: my-webapp
        tier: frontend
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}

controlplane ~ ✖ cat service.yml 
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: my-webapp
    tier: rontend
  name: front-end-service
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30083
  selector:
    app: my-webapp
    tier: frontend
  type: NodePort
status:
  loadBalancer: {}

=======================

Create a new Ingress Resource for the service my-video-service to be made available at the URL: http://ckad-mock-exam-solution.com:30093/video.


To create an ingress resource, the following details are: -

annotation: nginx.ingress.kubernetes.io/rewrite-target: /

host: ckad-mock-exam-solution.com

path: /video

Once set up, the curl test of the URL from the nodes should be successful: HTTP 200

http://ckad-mock-exam-solution.com:30093/video accessible?

  controlplane ~ ➜  k create ingress ingress-value --rule=ckad-mock-exam-solution.com/video=my-video-service:30093 --dry-run=client -o yaml > ingress.yml

======================

Create a job called whalesay with image docker/whalesay and command "cowsay I am going to ace CKAD!".

completions: 10

backoffLimit: 6

restartPolicy: Never


This simple job runs the popular cowsay game that was modifed by docker…

controlplane ~ ➜  k create job whalesay --image=docker/whalesay --dry-run=client -o yaml  -- "cowsay I am going to ace CKAD!" > job.yml

controlplane ~ ➜  cat job.yml 
apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  name: whalesay
spec:
  completions: 10
  backoffLimit: 6
  template:
    metadata:
      creationTimestamp: null
    spec:
      containers:
      - command:
        - echo
        - "cowsay I am going to ace CKAD!"
        image: docker/whalesay
        name: whalesay
        resources: {}
      restartPolicy: Never
status: {}


=========================

For this question, please set the context to cluster1 by running:


kubectl config use-context cluster1


On the cluster1, the team has installed multiple helm charts on a different namespace. By mistake, those deployed resources include one of the vulnerable images called kodekloud/click-counter:latest. Find out the release name and uninstall it.


info_outline
Solution
Run the following command to change the context: -

kubectl config use-context cluster1


In this task, we will use the helm commands and jq tool. Here are the steps: -



Run the helm ls command with -A option to list the releases deployed on all the namespaces using helm.
helm ls -A


We will use the jq tool to extract the image name from the deployments.
kubectl get deploy -n <NAMESPACE> <DEPLOYMENT-NAME> -o json | jq -r '.spec.template.spec.containers[].image'


Replace <NAMESPACE> with the namespace and <DEPLOYMENT-NAME> with the deployment name, which we get from the previous commands.

After finding the kodekloud/click-counter:latest image, use the helm uninstall to remove the deployed chart that are using this vulnerable image.


helm uninstall <RELEASE-NAME> -n <NAMESPACE

==================================================




For this question, please set the context to cluster3 by running:


kubectl config use-context cluster3


Our new client wants to deploy the resources through the popular Helm tool. In the initial phase, our team lead wants to deploy nginx, a very powerful and versatile web server software that is widely used to serve static content, reverse proxying, load balancing, from the bitnami helm chart on the cluster3-controlplane node.

The chart URL and other specifications are as follows: -


1. The chart URL link - https://charts.bitnami.com/bitnami

2. The chart repository name should be polar.

3. The release name should be nginx-server.

4. All the resources should be deployed on the cd-tool-apd namespace.


NOTE: - You have to perform this task from the student-node.

info_outline
Solution
Run the following command to change the context: -

kubectl config use-context cluster3


In this task, we will use the helm commands. Here are the steps: -


Add the repostiory to Helm with the following command: -

helm repo add polar https://charts.bitnami.com/bitnami


The helm repo add command is used to add a new chart repository to Helm and this allows us to browse and install charts from the new repository using the Helm package manager.



Use the helm repo ls command which is used to list all the currently configured Helm chart repositories on Kubernetes cluster.

helm repo ls 


Search for the nginx chart in a polar chart repository as follows: -

helm search repo polar | grep nginx



The helm search repo command is used to search for charts in a specified Helm chart repository. Also, it allows us to browse available charts and view their details, such as their name, version, description, and maintainers.


Before installing the chart, we have to create a namespace as given in the task description. Then we can install the nginx chart on a Kubernetes cluster.

kubectl create ns cd-tool-apd

helm install nginx-server polar/nginx -n cd-tool-apd

=============================================


CTION: APPLICATION DEPLOYMENT


For this question, please set the context to cluster3 by running:


kubectl config use-context cluster3


One co-worker deployed an nginx helm chart on the cluster3 server called lvm-crystal-apd. A new update is pushed to the helm chart, and the team wants you to update the helm repository to fetch the new changes.


After updating the helm chart, upgrade the helm chart version to above 13.2.9 and increase the replica count to 2.


NOTE: - We have to perform this task on the cluster3-controlplane node.


You can SSH into the cluster3 using ssh cluster3-controlplane command.

info_outline
Solution
Run the following command to change the context: -


kubectl config use-context cluster3


In this task, we will use the kubectl and helm commands. Here are the steps: -



Log in to the cluster3-controlplane node first and use the helm ls command to list all the releases installed using Helm in the Kubernetes cluster.

helm ls -A


Here -A or --all-namespaces option lists all the releases of all the namespaces.



Identify the namespace where the resources get deployed.


Use the helm repo ls command to list the helm repositories.
helm repo ls 


Now, update the helm repository with the following command: -

helm repo update lvm-crystal-apd -n crystal-apd-ns


The above command updates the local cache of available charts from the configured chart repositories.



The helm search command searches for all the available charts in a specific Helm chart repository. In our case, it's the nginx helm chart.
helm search repo lvm-crystal-apd/nginx -n crystal-apd-ns -l | head -n30


The -l or --versions option is used to display information about all available chart versions.



Upgrade the helm chart to above 13.2.9 and also, increase the replica count of the deployment to 2 from the command line. Use the helm upgrade command as follows: -

helm upgrade lvm-crystal-apd lvm-crystal-apd/nginx -n crystal-apd-ns --version=13.2.12 --set replicaCount=2


After upgrading the chart version, you can verify it with the following command: -

helm ls -n crystal-apd-ns


Look under the CHART column for the chart version.



Use the kubectl get command to check the replicas of the deployment: -
kubectl get deploy -n crystal-apd-ns


The available count 2 is under the AVAILABLE colum



---------------------------------------------------


r this question, please set the context to cluster1 by running:


kubectl config use-context cluster1


Create a pod named ckad17-qos-aecs-3 in namespace ckad17-nqoss-aecs with image nginx and container name ckad17-qos-ctr-3-aecs.

Define other fields such that the Pod is configured to use the Quality of Service (QoS) class of Burstable.


Also retrieve the name and QoS class of each Pod in the namespace ckad17-nqoss-aecs in the below format and save the output to a file named qos_status_aecs in the /root directory.


Format:

NAME    QOS
pod-1   qos_class
pod-2   qos_class
info_outline
Solution
student-node ~ ➜ kubectl config use-context cluster1
Switched to context "cluster1".

student-node ~ ➜  cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: ckad17-qos-aecs-3
  namespace: ckad17-nqoss-aecs
spec:
  containers:
  - name: ckad17-qos-ctr-3-aecs
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"
EOF

pod/ckad17-qos-aecs-3 created

student-node ~ ➜  kubectl --namespace=ckad17-nqoss-aecs get pod --output=custom-columns="NAME:.metadata.name,QOS:.status.qosClass"
NAME                QOS
ckad17-qos-aecs-1   BestEffort
ckad17-qos-aecs-2   Guaranteed
ckad17-qos-aecs-3   Burstable

student-node ~ ➜  kubectl --namespace=ckad17-nqoss-aecs get pod --output=custom-columns="NAME:.metadata.name,QOS:.status.qosClass" > /root/qos_status_aecs

=========================================





For this question, please set the context to cluster3 by running:


kubectl config use-context cluster3

We have deployed some pods in the namespaces ckad-alpha and ckad-beta.

You need to create a NetworkPolicy named ns-netpol-ckad that will restrict all Pods in Namespace ckad-alpha to only have outgoing traffic to Pods in Namespace ckad-beta . Ingress traffic should not be affected.


However, the NetworkPolicy you create should allow egress traffic on port 53 TCP and UDP.


info_outline
Solution
The following manifest will restrict all the pods in ckad-alpha namespace to only have egress traffic to pods in namespace ckad-beta. But it will allow egress on port 53.

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ns-netpol-ckad
  namespace: ckad-alpha
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - ports:
    - port: 53
      protocol: TCP
    - port: 53
      protocol: UDP
  - to:
     - namespaceSelector:
        matchLabels:
         kubernetes.io/metadata.name: ckad-beta


=============================

or this question, please set the context to cluster1 by running:


kubectl config use-context cluster1


Create a ResourceQuota called ckad19-rqc-aecs in the namespace ckad19-rqc-ns-aecs and enforce a limit of one ResourceQuota for the namespace.

info_outline
Solution
student-node ~ ➜  kubectl config use-context cluster1
Switched to context "cluster1".

student-node ~ ➜  kubectl create namespace ckad19-rqc-ns-aecs
namespace/ckad19-rqc-ns-aecs created

student-node ~ ➜  cat << EOF | kubectl apply -f -
apiVersion: v1
kind: ResourceQuota
metadata:
  name: ckad19-rqc-aecs
  namespace: ckad19-rqc-ns-aecs
spec:
  hard:
    resourcequotas: "1"
EOF

resourcequota/ckad19-rqc-aecs created

student-node ~ ➜  k get resourcequotas -n ckad19-rqc-ns-aecs
NAME              AGE   REQUEST               LIMIT
ckad19-rqc-aecs   20s   resourcequotas: 1/1   

==============




Create a Kubernetes LimitRange object named ckad15-memlt-aecs. This object limits the amount of memory each container in the namespace ckad15-memlt-ns-aecs can use.


The default memory limit should be 512Mi, and the default memory request should be 256Mi. Ensure that the manifest applies to container resources only.


Note: You may need to create or delete resources to complete this task.

info_outline
Solution
student-node ~ ➜  kubectl config use-context cluster1
Switched to context "cluster1".

student-node ~ ➜  kubectl create namespace ckad15-memlt-ns-aecs
namespace/ckad15-memlt-ns-aecs created

student-node ~ ➜  cat << EOF | kubectl apply -f - -n ckad15-memlt-ns-aecs
apiVersion: v1
kind: LimitRange
metadata:
  name: ckad15-memlt-aecs
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container
EOF

limitrange/ckad15-memlt-aecs created
=======================





For this scenario, create an ingress controller.


We have already deployed some of the required resources (Namespaces, Service accounts, Roles and Rolebindings).

Your task is to create the Ingress controller Deployment using the manifest given at /root/nginx-controller.yaml. There are some issues in the configuration. Please find the issues and fix them.


Note: All the resources are deployed in ingress-nginx namespace.

info_outline
Solution
Use cat command to view the contents of the file. cat /root/nginx-controller.yaml


Please check the apiVersion, resource kind, namespace and the container port sections
apiVersion: apps/betav1 #wrong api version
kind: deployment #change this
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.4
  name: ingress-nginx-controller
  namespace: ingressnginx  #issue1 ingress-nginx
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --publish-service=/ingress-nginx-controller
        - --election-id=ingress-nginx-leader
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: registry.k8s.io/ingress-nginx/controller:v1.6.4@sha256:15be4666c53052484dd2992efacf2f50ea77a78ae8aa21ccd91af6baaa7ea22f
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        -    containerPort: 80 #wrong indentation 
          name: http
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          runAsUser: 101
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission


  
  ==========================

  
For this question, please set the context to cluster3 by running:


kubectl config use-context cluster3


Define a Kubernetes custom resource definition (CRD) for a new resource kind called Foo (plural form - foos) in the samplecontroller.k8s.io group.

This CRD should have a version of v1alpha1 with a schema that includes two properties as given below:


deploymentName (a string type) and replicas (an integer type with minimum value of 1 and maximum value of 5).



It should also include a status subresource which enables retrieving and updating the status of Foo object, including the availableReplicas property, which is an integer type.
The Foo resource should be namespace scoped.


Note: We have provided a template /root/foo-crd-aecs.yaml for your ease. There are few issues with it so please make sure to incorporate the above requirements before deploying on cluster.

info_outline
Solution
student-node ~ ➜  kubectl config use-context cluster3
Switched to context "cluster3".

student-node ~ ➜  vim foo-crd-aecs.yaml

student-node ~ ➜  cat foo-crd-aecs.yaml 
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: foos.samplecontroller.k8s.io
  annotations:
    "api-approved.kubernetes.io": "unapproved, experimental-only"
spec:
  group: samplecontroller.k8s.io
  scope: Namespaced
  names:
    kind: Foo
    plural: foos
  versions:
    - name: v1alpha1
      served: true
      storage: true
      schema:
        # schema used for validation
        openAPIV3Schema:
          type: object
          properties:
            spec:
              # Spec for schema goes here !
              type: object
              properties:
                deploymentName:
                  type: string
                replicas:
                  type: integer
                  minimum: 1
                  maximum: 5
            status:
              type: object
              properties:
                availableReplicas:
                  type: integer
      # subresources for the custom resource
      subresources:
        # enables the status subresource
        status: {}

student-node ~ ➜  kubectl apply -f foo-crd-aecs.yaml
customresourcedefinition.apiextensions.k8s.io/foos.samplecontroller.k8s.io created

=======================================

SECTION: SERVICES AND NETWORKING
For this question, please set the context to cluster3 by running:


kubectl config use-context cluster3

We have an external webserver running on student-node which is exposed at port 9999.

We have also created a service called external-webserver-ckad01-svcn that can connect to our local webserver from within the cluster3 but, at the moment, it is not working as expected.



Fix the issue so that other pods within cluster3 can use external-webserver-ckad01-svcn service to access the webserver.


info_outline
Solution
Let's check if the webserver is working or not:


student-node ~ ➜  curl student-node:9999
...
<h1>Welcome to nginx!</h1>
...



Now we will check if service is correctly defined:

student-node ~ ➜  kubectl describe svc external-webserver-ckad01-svcn 
Name:              external-webserver-ckad01-svcn
Namespace:         default
.
.
Endpoints:         <none> # there are no endpoints for the service
...



As we can see there is no endpoints specified for the service, hence we won't be able to get any output. Since we can not destroy any k8s object, let's create the endpoint manually for this service as shown below:


student-node ~ ➜  export IP_ADDR=$(ifconfig eth0 | grep inet | awk '{print $2}')

student-node ~ ➜ kubectl --context cluster3 apply -f - <<EOF
apiVersion: v1
kind: Endpoints
metadata:
  # the name here should match the name of the Service
  name: external-webserver-ckad01-svcn
subsets:
  - addresses:
      - ip: $IP_ADDR
    ports:
      - port: 9999
EOF



Finally check if the curl test works now:

student-node ~ ➜  kubectl --context cluster3 run --rm  -i test-curl-pod --image=curlimages/curl --restart=Never -- curl -m 2 external-webserver-ckad01-svcn
...
<title>Welcome to nginx!</title>
...

=========================================

For this question, please set the context to cluster3 by running:


kubectl config use-context cluster3


One co-worker deployed an nginx helm chart on the cluster3 server called lvm-crystal-apd. A new update is pushed to the helm chart, and the team wants you to update the helm repository to fetch the new changes.


After updating the helm chart, upgrade the helm chart version to above 13.2.9 and increase the replica count to 2.


NOTE: - We have to perform this task on the cluster3-controlplane node.


You can SSH into the cluster3 using ssh cluster3-controlplane command.

info_outline
Solution
Run the following command to change the context: -


kubectl config use-context cluster3


In this task, we will use the kubectl and helm commands. Here are the steps: -



Log in to the cluster3-controlplane node first and use the helm ls command to list all the releases installed using Helm in the Kubernetes cluster.

helm ls -A


Here -A or --all-namespaces option lists all the releases of all the namespaces.



Identify the namespace where the resources get deployed.


Use the helm repo ls command to list the helm repositories.
helm repo ls 


Now, update the helm repository with the following command: -

helm repo update lvm-crystal-apd -n crystal-apd-ns


The above command updates the local cache of available charts from the configured chart repositories.



The helm search command searches for all the available charts in a specific Helm chart repository. In our case, it's the nginx helm chart.
helm search repo lvm-crystal-apd/nginx -n crystal-apd-ns -l | head -n30


The -l or --versions option is used to display information about all available chart versions.



Upgrade the helm chart to above 13.2.9 and also, increase the replica count of the deployment to 2 from the command line. Use the helm upgrade command as follows: -

helm upgrade lvm-crystal-apd lvm-crystal-apd/nginx -n crystal-apd-ns --version=13.2.12 --set replicaCount=2


After upgrading the chart version, you can verify it with the following command: -

helm ls -n crystal-apd-ns


Look under the CHART column for the chart version.



Use the kubectl get command to check the replicas of the deployment: -
kubectl get deploy -n crystal-apd-ns


The available count 2 is under the AVAILABLE column.

------------------------------------------------------

For this question, please set the context to cluster2 by running:


kubectl config use-context cluster2


On the cluster2-controlplane node, a Helm chart repository is given under the /opt/ path. It contains the files that describe a set of Kubernetes resources that can be deployed as a single unit. The files have some issues. Fix those issues and deploy them with the following specifications: -


1. The release name should be webapp-color-apd.

2. All the resources should be deployed on the frontend-apd namespace.
3. The service type should be node port.

4. Scale the deployment to 3.

5. Application version should be 1.20.0.


NOTE: - Remember to make necessary changes in the values.yaml and Chart.yaml files according to the specifications, and, to fix the issues, inspect the template files.

You can SSH into the cluster2 using ssh cluster2-controlplane command.

info_outline
Solution
Run the following command to change the context: -

kubectl config use-context cluster2


In this task, we will use the helm commands. Here are the steps: -



First, check the given namespace; if it doesn't exist, we must create it first; otherwise, it will give an error "namespaces not found" while installing the helm chart.
To check all the namespaces in the cluster2, we would have to run the following command: -

kubectl get ns


It will list all the namespaces. If the given namespace doesn't exist, then run the following command: -

kubectl create ns frontend-apd
Now, SSH to the cluster2-controlplane node and go to the /opt/ directory. We have given the helm chart directory webapp-color-apd that contains templates, values files, and the chart file etc.
Update the values according to the given specifications as follows: -

a.) Update the value of the appVersion to 1.20.0 in the Chart.yaml file.

b.) Update the value of the replicaCount to 3 in the values.yaml file.

c.) Update the value of the type to NodePort in the values.yaml file.
These are the values we have to update.

Now, we will use the helm lint command to check the Helm chart because it can identify errors such as missing or misconfigured values, invalid YAML syntax, and deprecated APIs etc.
cd /opt/

helm lint ./webapp-color-apd/


If there is no misconfiguration, we will see the similar output: -

helm lint ./webapp-color-apd/
==> Linting ./webapp-color-apd/
[INFO] Chart.yaml: icon is recommended

1 chart(s) linted, 0 chart(s) failed


But in our case, there are some issues with the given templates.


Deployment apiVersion needs to be correctly written. It should be apiVersion: apps/v1.

In the service YAML, there is a typo in the template variable {{ .Values.service.name }} because of that, it's not able to reference the value of the name field defined in the values.yaml file for the Kubernetes service that is being created or updated.


Now run the following command to install the helm chart in the frontend-apd namespace: -

helm install webapp-color-apd -n frontend-apd ./webapp-color-apd


Use the helm ls command to list the release deployed using helm.

helm ls -n frontend-apd

-----===================================================

For this question, please set the context to cluster3 by running:


kubectl config use-context cluster3


Define a Kubernetes custom resource definition (CRD) for a new resource kind called Foo (plural form - foos) in the samplecontroller.k8s.io group.

This CRD should have a version of v1alpha1 with a schema that includes two properties as given below:


deploymentName (a string type) and replicas (an integer type with minimum value of 1 and maximum value of 5).



It should also include a status subresource which enables retrieving and updating the status of Foo object, including the availableReplicas property, which is an integer type.
The Foo resource should be namespace scoped.


Note: We have provided a template /root/foo-crd-aecs.yaml for your ease. There are few issues with it so please make sure to incorporate the above requirements before deploying on cluster.

info_outline
Solution
student-node ~ ➜  kubectl config use-context cluster3
Switched to context "cluster3".

student-node ~ ➜  vim foo-crd-aecs.yaml

student-node ~ ➜  cat foo-crd-aecs.yaml 
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: foos.samplecontroller.k8s.io
  annotations:
    "api-approved.kubernetes.io": "unapproved, experimental-only"
spec:
  group: samplecontroller.k8s.io
  scope: Namespaced
  names:
    kind: Foo
    plural: foos
  versions:
    - name: v1alpha1
      served: true
      storage: true
      schema:
        # schema used for validation
        openAPIV3Schema:
          type: object
          properties:
            spec:
              # Spec for schema goes here !
              type: object
              properties:
                deploymentName:
                  type: string
                replicas:
                  type: integer
                  minimum: 1
                  maximum: 5
            status:
              type: object
              properties:
                availableReplicas:
                  type: integer
      # subresources for the custom resource
      subresources:
        # enables the status subresource
        status: {}

student-node ~ ➜  kubectl apply -f foo-crd-aecs.yaml
customresourcedefinition.apiextensions.k8s.io/foos.samplecontroller.k8s.io created

=========================================================


For this question, please set the context to cluster1 by running:


kubectl config use-context cluster1


We have a sample CRD at /root/ckad10-crd-aecs.yaml which should have the following validations:


destinationName, country, and city must be string types.

pricePerNight must be an integer between 50 and 5000.

durationInDays must be an integer between 1 and 30.


Update the file incorporating the above validations in a namespaced scope.


Note: Remember to create the CRD after the required changes.

info_outline
Solution
student-node ~ ➜  kubectl config use-context cluster1
Switched to context "cluster1".

student-node ~ ➜  vim ckad10-crd-aecs.yaml 

student-node ~ ➜  cat ckad10-crd-aecs.yaml 
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: holidaydestinations.destinations.k8s.io
  annotations:
    "api-approved.kubernetes.io": "unapproved, experimental-only"
  labels:
    app: holiday
spec:
  group: destinations.k8s.io
  names:
    kind: HolidayDestination
    singular: holidaydestination
    plural: holidaydestinations
    shortNames:
      - hd
  scope: Namespaced
  versions:
    - name: v1alpha1
      served: true
      storage: true
      schema:
        # schema used for validation
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                destinationName:
                  type: string
                country:
                  type: string
                city:
                  type: string
                pricePerNight:
                  type: integer
                  minimum: 50
                  maximum: 5000
                durationInDays:
                  type: integer
                  minimum: 1
                  maximum: 30
            status:
              type: object
              properties:
                availableRooms:
                  type: integer
                  minimum: 0
                  maximum: 1000
      # subresources for the custom resource
      subresources:
        # enables the status subresource
        status: {}

student-node ~ ➜  k create -f ckad10-crd-aecs.yaml
customresourcedefinition.apiextensions.k8s.io/holidaydestinations.destinations.k8s.io created


=======================================


For this question, please set the context to cluster3 by running:


kubectl config use-context cluster3

This scenario is categorized into two parts. Please find them below.

Part I:

We have already deployed several pods in the default namespace. Create a ClusterIP service .i.e. service-3421-svcn, which should expose the pods, namely, pod-23 and pod-21, with port set to 8080 and targetport to 80.

Part II:
Store the pod names and their ip addresses from all namespaces at /root/pod_ips_ckad02_svcn where the output is sorted by their IPs.

Please ensure the format as shown below:



POD_NAME        IP_ADDR
pod-1           ip-1
pod-3           ip-2
pod-2           ip-3
...



info_outline
Solution
Switching to cluster3:



kubectl config use-context cluster3



The easiest way to route traffic to a specific pod is by the use of labels and selectors . List the pods along with their labels:



student-node ~ ➜  kubectl get pods --show-labels 
NAME     READY   STATUS    RESTARTS   AGE     LABELS
pod-12   1/1     Running   0          5m21s   env=dev,mode=standard,type=external
pod-34   1/1     Running   0          5m20s   env=dev,mode=standard,type=internal
pod-43   1/1     Running   0          5m20s   env=prod,mode=exam,type=internal
pod-23   1/1     Running   0          5m21s   env=dev,mode=exam,type=external
pod-32   1/1     Running   0          5m20s   env=prod,mode=standard,type=internal
pod-21   1/1     Running   0          5m20s   env=prod,mode=exam,type=external



Looks like there are a lot of pods created to confuse us. But we are only concerned with the labels of pod-23 and pod-21.



As we can see both the required pods have labels mode=exam,type=external in common. Let's confirm that using kubectl too:



student-node ~ ➜  kubectl get pod -l mode=exam,type=external                                       
NAME     READY   STATUS    RESTARTS   AGE
pod-23   1/1     Running   0          9m18s
pod-21   1/1     Running   0          9m17s



Nice!! Now as we have figured out the labels, we can proceed further with the creation of the service:



student-node ~ ➜  kubectl create service clusterip service-3421-svcn --tcp=8080:80 --dry-run=client -o yaml > service-3421-svcn.yaml



Now modify the service definition with selectors as required before applying to k8s cluster:



student-node ~ ➜  cat service-3421-svcn.yaml 
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: service-3421-svcn
  name: service-3421-svcn
spec:
  ports:
  - name: 8080-80
    port: 8080
    protocol: TCP
    targetPort: 80
  selector:
    app: service-3421-svcn  # delete 
    mode: exam    # add
    type: external  # add
  type: ClusterIP
status:
  loadBalancer: {}



Finally let's apply the service definition:



student-node ~ ➜  kubectl apply -f service-3421-svcn.yaml
service/service-3421 created

student-node ~ ➜  k get ep service-3421-svcn 
NAME           ENDPOINTS                     AGE
service-3421   10.42.0.15:80,10.42.0.17:80   52s



To store all the pod name along with their IP's , we could use imperative command as shown below:



student-node ~ ➜  kubectl get pods -A -o=custom-columns='POD_NAME:metadata.name,IP_ADDR:status.podIP' --sort-by=.status.podIP

POD_NAME                                  IP_ADDR
helm-install-traefik-crd-lbwzr            10.42.0.2
local-path-provisioner-7b7dc8d6f5-d4x7t   10.42.0.3
metrics-server-668d979685-vh7bk           10.42.0.4
...

# store the output to /root/pod_ips_ckad02_svcn
student-node ~ ➜  kubectl get pods -A -o=custom-columns='POD_NAME:metadata.name,IP_ADDR:status


=================================

For this question, please set the context to cluster2 by running:


kubectl config use-context cluster2


Create a custom resource my-anime of kind Anime with the below specifications:


Name of Anime: Death Note
Episode Count: 37


TIP: You may find the respective CRD with anime substring in it.

info_outline
Solution
student-node ~ ➜  kubectl config use-context cluster2
Switched to context "cluster2".

student-node ~ ➜  kubectl get crd | grep -i anime
animes.animes.k8s.io

student-node ~ ➜  kubectl get crd animes.animes.k8s.io \
                 -o json \
                 | jq .spec.versions[].schema.openAPIV3Schema.properties.spec.properties
{
  "animeName": {
    "type": "string"
  },
  "episodeCount": {
    "maximum": 52,
    "minimum": 24,
    "type": "integer"
  }
}

student-node ~ ➜  k api-resources | grep anime
animes                            an           animes.k8s.io/v1alpha1                 true         Anime

student-node ~ ➜  cat << YAML | kubectl apply -f -
 apiVersion: animes.k8s.io/v1alpha1
 kind: Anime
 metadata:
   name: my-anime
 spec:
   animeName: "Death Note"
   episodeCount: 37
YAML
anime.animes.k8s.io/my-anime created

student-node ~ ➜  k get an my-anime 
NAME       AGE
my-anime   23s
====================================

For this question, please set the context to cluster3 by running:


kubectl config use-context cluster3


We have created a Network Policy netpol-ckad13-svcn that allows traffic only to specific pods and it allows traffic only from pods with specific labels.

Your task is to edit the policy so that it allows traffic from pods with labels access = allowed.



Do not change the existing rules in the policy.

info_outline
Solution
To edit the existing network policy use the following command:

kubectl edit netpol netpol-ckad13-svcn


Edit the policy as follows:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: netpol-ckad13-svcn
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: kk-app
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
           tier: server
#add the following in the manifest
    - podSelector:
        matchLabels:
           access: allowed

=====================================





For this question, please set the context to cluster3 by running:


kubectl config use-context cluster3

We have deployed some pods in the namespaces ckad-alpha and ckad-beta.

You need to create a NetworkPolicy named ns-netpol-ckad that will restrict all Pods in Namespace ckad-alpha to only have outgoing traffic to Pods in Namespace ckad-beta . Ingress traffic should not be affected.


However, the NetworkPolicy you create should allow egress traffic on port 53 TCP and UDP.


info_outline
Solution
The following manifest will restrict all the pods in ckad-alpha namespace to only have egress traffic to pods in namespace ckad-beta. But it will allow egress on port 53.

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ns-netpol-ckad
  namespace: ckad-alpha
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - ports:
    - port: 53
      protocol: TCP
    - port: 53
      protocol: UDP
  - to:
     - namespaceSelector:
        matchLabels:
         kubernetes.io/metadata.name: ckad-beta

============================================



For this question, please set the context to cluster1 by running:


kubectl config use-context cluster1


We have a sample CRD at /root/ckad10-crd-aecs.yaml which should have the following validations:


destinationName, country, and city must be string types.

pricePerNight must be an integer between 50 and 5000.

durationInDays must be an integer between 1 and 30.


Update the file incorporating the above validations in a namespaced scope.


Note: Remember to create the CRD after the required changes.

info_outline
Solution
student-node ~ ➜  kubectl config use-context cluster1
Switched to context "cluster1".

student-node ~ ➜  vim ckad10-crd-aecs.yaml 

student-node ~ ➜  cat ckad10-crd-aecs.yaml 
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: holidaydestinations.destinations.k8s.io
  annotations:
    "api-approved.kubernetes.io": "unapproved, experimental-only"
  labels:
    app: holiday
spec:
  group: destinations.k8s.io
  names:
    kind: HolidayDestination
    singular: holidaydestination
    plural: holidaydestinations
    shortNames:
      - hd
  scope: Namespaced
  versions:
    - name: v1alpha1
      served: true
      storage: true
      schema:
        # schema used for validation
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                destinationName:
                  type: string
                country:
                  type: string
                city:
                  type: string
                pricePerNight:
                  type: integer
                  minimum: 50
                  maximum: 5000
                durationInDays:
                  type: integer
                  minimum: 1
                  maximum: 30
            status:
              type: object
              properties:
                availableRooms:
                  type: integer
                  minimum: 0
                  maximum: 1000
      # subresources for the custom resource
      subresources:
        # enables the status subresource
        status: {}

student-node ~ ➜  k create -f ckad10-crd-aecs.yaml
customresourcedefinition.apiextensions.k8s.io/holidaydestinations.destinations.k8s.io creat

============================


For this question, please set the context to cluster3 by running:


kubectl config use-context cluster3


A new payment service has been introduced. Since it is a sensitive application, it is deployed in its own namespace critical-space. Inspect the resources and service created.


You are requested to make the new application available at /pay. Create an ingress resource named ingress-ckad09-svcn for the payment application to make it available at /pay



Identify and implement the best approach to making this application available on the ingress controller and test to make sure its working. Look into annotations: rewrite-target as well.

info_outline
Solution
Switch to Cluster3 using the following:

kubectl config use-context cluster3
Solution manifest file to create a new ingress service to make the application available at /pay as follows:

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-ckad09-svcn
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /pay
        pathType: Prefix
        backend:
          service:
           name: pay-service
           port:
            number: 8282